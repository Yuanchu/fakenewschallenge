{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from csv import DictReader\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import concatenate, Embedding, Dense, Dropout, Activation, LSTM, CuDNNLSTM, CuDNNGRU,Flatten, Input, RepeatVector, TimeDistributed, Bidirectional\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "import codecs\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_HEAD = 100\n",
    "MAX_LEN_BODY = 500\n",
    "VOCAB_SIZE = 15000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(lst, vocab_size):\n",
    "    \"\"\"\n",
    "    lst: list of sentences\n",
    "    \"\"\"\n",
    "    vocabcount = Counter(w for txt in lst for w in txt.lower().split())\n",
    "    vocabcount = vocabcount.most_common(vocab_size)\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    for i, word in enumerate(vocabcount):\n",
    "        word2idx[word[0]] = i\n",
    "        idx2word[i] = word[0]\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov2idx_unk(lst, word2idx):\n",
    "    output = []\n",
    "    for sentence in lst:\n",
    "        temp = []\n",
    "        for word in sentence.split():\n",
    "            if word in word2idx:\n",
    "                temp.append(word2idx[word])\n",
    "            else:\n",
    "                temp.append(word2idx['<unk>'])\n",
    "        temp.append(word2idx['<unk>'])\n",
    "        output.append(temp)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(cov_lst, max_len=MAX_LEN_BODY):\n",
    "    \"\"\"\n",
    "    list of list of index converted from words\n",
    "    \"\"\"\n",
    "    pad_lst = pad_sequences(cov_lst, maxlen = max_len, padding='post')\n",
    "    return pad_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ref = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_unk(file_instances, file_bodies):\n",
    "    \"\"\"\n",
    "    article: the name of the article file\n",
    "    \"\"\"\n",
    "    \n",
    "    instance_lst = []\n",
    "    # Process file\n",
    "    with open(file_instances, \"r\", encoding='utf-8') as table:\n",
    "        r = DictReader(table)\n",
    "        for line in r:\n",
    "            instance_lst.append(line)\n",
    "            \n",
    "    body_lst = []\n",
    "    # Process file\n",
    "    with open(file_bodies, \"r\", encoding='utf-8') as table:\n",
    "        r = DictReader(table)\n",
    "        for line in r:\n",
    "            body_lst.append(line)\n",
    "    \n",
    "    heads = {}\n",
    "    bodies = {}\n",
    "    \n",
    "    for instance in instance_lst:\n",
    "        if instance['Headline'] not in heads:\n",
    "            head_id = len(heads)\n",
    "            heads[instance['Headline']] = head_id\n",
    "        instance['Body ID'] = int(instance['Body ID'])\n",
    "    for body in body_lst:\n",
    "        bodies[int(body['Body ID'])] = body['articleBody']\n",
    "            \n",
    "    headData = []\n",
    "    bodyData = []\n",
    "    labelData = []\n",
    "    for instance in instance_lst:\n",
    "        headData.append(instance['Headline'])\n",
    "        bodyData.append(bodies[instance['Body ID']])\n",
    "        labelData.append(label_ref[instance['Stance']])\n",
    "            \n",
    "    \n",
    "    word2idx, idx2word = get_vocab(headData+bodyData, VOCAB_SIZE)\n",
    "    word2idx['<unk>'] = len(word2idx)\n",
    "    \n",
    "    cov_head = cov2idx_unk(headData, word2idx)\n",
    "    cov_body = cov2idx_unk(bodyData, word2idx)\n",
    "    remove_list = []\n",
    "    for i in range(len(cov_head)):\n",
    "        if len(cov_head[i])>MAX_LEN_HEAD or len(cov_body[i])>MAX_LEN_BODY:\n",
    "            remove_list.append(i)\n",
    "    for idx in sorted(remove_list, reverse = True):\n",
    "        cov_head.pop(idx)\n",
    "        cov_body.pop(idx)\n",
    "        labelData.pop(idx)\n",
    "    pad_head = pad_seq(cov_head, MAX_LEN_HEAD)\n",
    "    pad_body = pad_seq(cov_body, MAX_LEN_BODY)\n",
    "    return pad_head, pad_body, labelData, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_head, pad_body, labelData, word2idx, idx2word = load_train_unk(\"train_stances.csv\", \"train_bodies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training\n",
    "train_head = pad_head[:-1000]\n",
    "train_body = pad_body[:-1000]\n",
    "train_label = labelData[:-1000]\n",
    "\n",
    "val_head = pad_head[-1000:]\n",
    "val_body = pad_body[-1000:]\n",
    "val_label = labelData[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_LAYERS = 0\n",
    "HIDDEN_DIM =  512\n",
    "EPOCHS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_head (InputLayer)         (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_body (InputLayer)         (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 100, 300)     4500300     input_head[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 500, 300)     4500300     input_body[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_5 (CuDNNGRU)          (None, 128)          165120      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_6 (CuDNNGRU)          (None, 128)          165120      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           cu_dnngru_5[0][0]                \n",
      "                                                                 cu_dnngru_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 400)          102800      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 400)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 400)          160400      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 400)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 4)            1604        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 9,595,644\n",
      "Trainable params: 9,595,644\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_head = Input(shape=(MAX_LEN_HEAD,), dtype='int32', name='input_head')\n",
    "embed_head = Embedding(output_dim=EMBEDDING_DIM, input_dim=VOCAB_SIZE+1, input_length=MAX_LEN_HEAD)(input_head)\n",
    "gru_head = CuDNNGRU(128)(embed_head)\n",
    "# embed_head = Embedding(VOCAB_SIZE, EMBEDDING_DIM , input_length = MAX_LEN_HEAD, weights = [g_word_embedding_matrix], trainable=False)\n",
    "input_body = Input(shape=(MAX_LEN_BODY,), dtype='int32', name='input_body')\n",
    "embed_body = Embedding(output_dim=EMBEDDING_DIM, input_dim=VOCAB_SIZE+1, input_length=MAX_LEN_BODY)(input_body)\n",
    "gru_body = CuDNNGRU(128)(embed_body)\n",
    "# embed_body = Embedding(VOCAB_SIZE, EMBEDDING_DIM , input_length = MAX_LEN_BODY, weights = [g_word_embedding_matrix], trainable=False)\n",
    "\n",
    "concat = concatenate([gru_head, gru_body], axis = 1)\n",
    "\n",
    "x = Dense(400, activation='relu')(concat)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(400, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# And finally we add the main logistic regression layer\n",
    "main_output = Dense(4, activation='softmax', name='main_output')(x)\n",
    "model = Model(inputs=[input_head, input_body], outputs=main_output)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38357 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.8110 - acc: 0.7388 - val_loss: 0.7872 - val_acc: 0.7400\n",
      "Epoch 2/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7966 - acc: 0.7401 - val_loss: 0.7754 - val_acc: 0.7400\n",
      "Epoch 3/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7935 - acc: 0.7401 - val_loss: 0.7870 - val_acc: 0.7400\n",
      "Epoch 4/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7915 - acc: 0.7401 - val_loss: 0.7723 - val_acc: 0.7400\n",
      "Epoch 5/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7916 - acc: 0.7401 - val_loss: 0.7745 - val_acc: 0.7400\n",
      "Epoch 6/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7916 - acc: 0.7401 - val_loss: 0.7730 - val_acc: 0.7400\n",
      "Epoch 7/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7908 - acc: 0.7401 - val_loss: 0.7733 - val_acc: 0.7400\n",
      "Epoch 8/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7916 - acc: 0.7401 - val_loss: 0.7745 - val_acc: 0.7400\n",
      "Epoch 9/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7907 - acc: 0.7401 - val_loss: 0.7733 - val_acc: 0.7400\n",
      "Epoch 10/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7912 - acc: 0.7401 - val_loss: 0.7695 - val_acc: 0.7400\n",
      "Epoch 11/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.7903 - acc: 0.7400 - val_loss: 0.7773 - val_acc: 0.7400\n",
      "Epoch 12/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.7896 - acc: 0.7411 - val_loss: 0.7713 - val_acc: 0.7400\n",
      "Epoch 13/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.7879 - acc: 0.7411 - val_loss: 0.7792 - val_acc: 0.7390\n",
      "Epoch 14/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7894 - acc: 0.7403 - val_loss: 0.7704 - val_acc: 0.7420\n",
      "Epoch 15/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.7863 - acc: 0.7420 - val_loss: 0.7744 - val_acc: 0.7380\n",
      "Epoch 16/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.7802 - acc: 0.7443 - val_loss: 0.7513 - val_acc: 0.7440\n",
      "Epoch 17/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.7459 - acc: 0.7582 - val_loss: 0.7076 - val_acc: 0.7670\n",
      "Epoch 18/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.6624 - acc: 0.7855 - val_loss: 0.6208 - val_acc: 0.7840\n",
      "Epoch 19/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.6172 - acc: 0.7959 - val_loss: 0.6101 - val_acc: 0.8000\n",
      "Epoch 20/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5948 - acc: 0.8058 - val_loss: 0.5853 - val_acc: 0.7990\n",
      "Epoch 21/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5732 - acc: 0.8118 - val_loss: 0.5496 - val_acc: 0.7980\n",
      "Epoch 22/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.5569 - acc: 0.8148 - val_loss: 0.5480 - val_acc: 0.8070\n",
      "Epoch 23/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.5456 - acc: 0.8173 - val_loss: 0.5618 - val_acc: 0.8010\n",
      "Epoch 24/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5413 - acc: 0.8172 - val_loss: 0.5403 - val_acc: 0.8040\n",
      "Epoch 25/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5343 - acc: 0.8169 - val_loss: 0.5073 - val_acc: 0.8130\n",
      "Epoch 26/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.5187 - acc: 0.8195 - val_loss: 0.5119 - val_acc: 0.8150\n",
      "Epoch 27/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.5151 - acc: 0.8196 - val_loss: 0.5142 - val_acc: 0.8100\n",
      "Epoch 28/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5108 - acc: 0.8208 - val_loss: 0.5196 - val_acc: 0.8040\n",
      "Epoch 29/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5077 - acc: 0.8210 - val_loss: 0.5144 - val_acc: 0.8190\n",
      "Epoch 30/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5062 - acc: 0.8210 - val_loss: 0.5189 - val_acc: 0.8120\n",
      "Epoch 31/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5057 - acc: 0.8218 - val_loss: 0.4967 - val_acc: 0.8090\n",
      "Epoch 32/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5030 - acc: 0.8203 - val_loss: 0.4925 - val_acc: 0.8140\n",
      "Epoch 33/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5025 - acc: 0.8207 - val_loss: 0.5311 - val_acc: 0.8070\n",
      "Epoch 34/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5007 - acc: 0.8228 - val_loss: 0.5270 - val_acc: 0.8150\n",
      "Epoch 35/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5004 - acc: 0.8212 - val_loss: 0.4965 - val_acc: 0.8190\n",
      "Epoch 36/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.5004 - acc: 0.8212 - val_loss: 0.4994 - val_acc: 0.8080\n",
      "Epoch 37/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5024 - acc: 0.8210 - val_loss: 0.5056 - val_acc: 0.8100\n",
      "Epoch 38/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.4997 - acc: 0.8209 - val_loss: 0.4901 - val_acc: 0.8110\n",
      "Epoch 39/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4985 - acc: 0.8212 - val_loss: 0.4925 - val_acc: 0.8070\n",
      "Epoch 40/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4957 - acc: 0.8220 - val_loss: 0.5127 - val_acc: 0.8160\n",
      "Epoch 41/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4989 - acc: 0.8210 - val_loss: 0.4985 - val_acc: 0.8140\n",
      "Epoch 42/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4973 - acc: 0.8206 - val_loss: 0.5243 - val_acc: 0.8050\n",
      "Epoch 43/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4983 - acc: 0.8187 - val_loss: 0.5506 - val_acc: 0.8190\n",
      "Epoch 44/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4977 - acc: 0.8210 - val_loss: 0.5095 - val_acc: 0.8060\n",
      "Epoch 45/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4978 - acc: 0.8196 - val_loss: 0.5002 - val_acc: 0.8120\n",
      "Epoch 46/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4984 - acc: 0.8194 - val_loss: 0.5176 - val_acc: 0.8010\n",
      "Epoch 47/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4998 - acc: 0.8209 - val_loss: 0.5113 - val_acc: 0.8150\n",
      "Epoch 48/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4989 - acc: 0.8192 - val_loss: 0.4979 - val_acc: 0.8110\n",
      "Epoch 49/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4985 - acc: 0.8190 - val_loss: 0.4981 - val_acc: 0.8160\n",
      "Epoch 50/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4991 - acc: 0.8198 - val_loss: 0.5061 - val_acc: 0.8090\n",
      "Epoch 51/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5005 - acc: 0.8202 - val_loss: 0.5007 - val_acc: 0.8040\n",
      "Epoch 52/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4993 - acc: 0.8198 - val_loss: 0.4911 - val_acc: 0.8130\n",
      "Epoch 53/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5064 - acc: 0.8201 - val_loss: 0.5176 - val_acc: 0.8090\n",
      "Epoch 54/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5005 - acc: 0.8196 - val_loss: 0.5008 - val_acc: 0.8040\n",
      "Epoch 55/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5014 - acc: 0.8199 - val_loss: 0.5040 - val_acc: 0.8130\n",
      "Epoch 56/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5001 - acc: 0.8192 - val_loss: 0.4934 - val_acc: 0.8130\n",
      "Epoch 57/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5023 - acc: 0.8183 - val_loss: 0.4842 - val_acc: 0.8080\n",
      "Epoch 58/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5013 - acc: 0.8201 - val_loss: 0.4993 - val_acc: 0.8100\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4990 - acc: 0.8194 - val_loss: 0.4951 - val_acc: 0.8180\n",
      "Epoch 60/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4962 - acc: 0.8200 - val_loss: 0.4984 - val_acc: 0.8150\n",
      "Epoch 61/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5005 - acc: 0.8191 - val_loss: 0.4993 - val_acc: 0.8150\n",
      "Epoch 62/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4991 - acc: 0.8195 - val_loss: 0.5217 - val_acc: 0.8130\n",
      "Epoch 63/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4969 - acc: 0.8183 - val_loss: 0.5119 - val_acc: 0.7990\n",
      "Epoch 64/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5011 - acc: 0.8192 - val_loss: 0.9617 - val_acc: 0.8040\n",
      "Epoch 65/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5040 - acc: 0.8188 - val_loss: 0.5393 - val_acc: 0.8130\n",
      "Epoch 66/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5004 - acc: 0.8196 - val_loss: 0.5186 - val_acc: 0.8070\n",
      "Epoch 67/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4998 - acc: 0.8193 - val_loss: 0.5366 - val_acc: 0.8100\n",
      "Epoch 68/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.4987 - acc: 0.8183 - val_loss: 0.6105 - val_acc: 0.8070\n",
      "Epoch 69/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5048 - acc: 0.8196 - val_loss: 0.5059 - val_acc: 0.8130\n",
      "Epoch 70/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5001 - acc: 0.8197 - val_loss: 0.4806 - val_acc: 0.8140\n",
      "Epoch 71/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5000 - acc: 0.8194 - val_loss: 0.5458 - val_acc: 0.7930\n",
      "Epoch 72/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5035 - acc: 0.8187 - val_loss: 0.4969 - val_acc: 0.8040\n",
      "Epoch 73/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5004 - acc: 0.8187 - val_loss: 0.4822 - val_acc: 0.8140\n",
      "Epoch 74/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5025 - acc: 0.8191 - val_loss: 0.4842 - val_acc: 0.8140\n",
      "Epoch 75/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5001 - acc: 0.8202 - val_loss: 0.4874 - val_acc: 0.8120\n",
      "Epoch 76/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5018 - acc: 0.8188 - val_loss: 0.5043 - val_acc: 0.8130\n",
      "Epoch 77/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5055 - acc: 0.8192 - val_loss: 0.4824 - val_acc: 0.8200\n",
      "Epoch 78/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5051 - acc: 0.8175 - val_loss: 0.4966 - val_acc: 0.8170\n",
      "Epoch 79/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5051 - acc: 0.8193 - val_loss: 0.4970 - val_acc: 0.8100\n",
      "Epoch 80/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5036 - acc: 0.8179 - val_loss: 0.5508 - val_acc: 0.7950\n",
      "Epoch 81/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5060 - acc: 0.8171 - val_loss: 0.5260 - val_acc: 0.7970\n",
      "Epoch 82/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5047 - acc: 0.8195 - val_loss: 0.4939 - val_acc: 0.8130\n",
      "Epoch 83/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5046 - acc: 0.8191 - val_loss: 0.5149 - val_acc: 0.8130\n",
      "Epoch 84/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5053 - acc: 0.8200 - val_loss: 0.5185 - val_acc: 0.8090\n",
      "Epoch 85/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5029 - acc: 0.8195 - val_loss: 0.5176 - val_acc: 0.8080\n",
      "Epoch 86/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5096 - acc: 0.8187 - val_loss: 0.5186 - val_acc: 0.8040\n",
      "Epoch 87/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5046 - acc: 0.8188 - val_loss: 0.5331 - val_acc: 0.8090\n",
      "Epoch 88/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5078 - acc: 0.8176 - val_loss: 0.5238 - val_acc: 0.8060\n",
      "Epoch 89/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5053 - acc: 0.8168 - val_loss: 0.4897 - val_acc: 0.8150\n",
      "Epoch 90/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5059 - acc: 0.8188 - val_loss: 0.4955 - val_acc: 0.8100\n",
      "Epoch 91/120\n",
      "38357/38357 [==============================] - 44s 1ms/step - loss: 0.5055 - acc: 0.8171 - val_loss: 0.5020 - val_acc: 0.8020\n",
      "Epoch 92/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5068 - acc: 0.8173 - val_loss: 0.4994 - val_acc: 0.8120\n",
      "Epoch 93/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5040 - acc: 0.8184 - val_loss: 0.4873 - val_acc: 0.8220\n",
      "Epoch 94/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5053 - acc: 0.8172 - val_loss: 0.5411 - val_acc: 0.8090\n",
      "Epoch 95/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5161 - acc: 0.8189 - val_loss: 0.4960 - val_acc: 0.8140\n",
      "Epoch 96/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5037 - acc: 0.8193 - val_loss: 0.4949 - val_acc: 0.8160\n",
      "Epoch 97/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5037 - acc: 0.8189 - val_loss: 0.4911 - val_acc: 0.8150\n",
      "Epoch 98/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5046 - acc: 0.8176 - val_loss: 0.4965 - val_acc: 0.8160\n",
      "Epoch 99/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5059 - acc: 0.8188 - val_loss: 0.4969 - val_acc: 0.8170\n",
      "Epoch 100/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5045 - acc: 0.8176 - val_loss: 0.5883 - val_acc: 0.8060\n",
      "Epoch 101/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5090 - acc: 0.8163 - val_loss: 0.4998 - val_acc: 0.8190\n",
      "Epoch 102/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5073 - acc: 0.8176 - val_loss: 0.5694 - val_acc: 0.7950\n",
      "Epoch 103/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5105 - acc: 0.8158 - val_loss: 0.4976 - val_acc: 0.8180\n",
      "Epoch 104/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5039 - acc: 0.8172 - val_loss: 0.4946 - val_acc: 0.8170\n",
      "Epoch 105/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5090 - acc: 0.8164 - val_loss: 0.5209 - val_acc: 0.8060\n",
      "Epoch 106/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5032 - acc: 0.8187 - val_loss: 0.5742 - val_acc: 0.8080\n",
      "Epoch 107/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5040 - acc: 0.8174 - val_loss: 0.4929 - val_acc: 0.8100\n",
      "Epoch 108/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5036 - acc: 0.8177 - val_loss: 0.5022 - val_acc: 0.8040\n",
      "Epoch 109/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5068 - acc: 0.8165 - val_loss: 0.5101 - val_acc: 0.8190\n",
      "Epoch 110/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5088 - acc: 0.8145 - val_loss: 0.5005 - val_acc: 0.8180\n",
      "Epoch 111/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5050 - acc: 0.8174 - val_loss: 0.5216 - val_acc: 0.8150\n",
      "Epoch 112/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5033 - acc: 0.8173 - val_loss: 0.4880 - val_acc: 0.8150\n",
      "Epoch 113/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5015 - acc: 0.8183 - val_loss: 0.5331 - val_acc: 0.7900\n",
      "Epoch 114/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5028 - acc: 0.8172 - val_loss: 0.5201 - val_acc: 0.8140\n",
      "Epoch 115/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5035 - acc: 0.8183 - val_loss: 0.5602 - val_acc: 0.8040\n",
      "Epoch 116/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5049 - acc: 0.8165 - val_loss: 0.5121 - val_acc: 0.8120\n",
      "Epoch 117/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5006 - acc: 0.8196 - val_loss: 0.5203 - val_acc: 0.8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5037 - acc: 0.8170 - val_loss: 0.5274 - val_acc: 0.8080\n",
      "Epoch 119/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5035 - acc: 0.8186 - val_loss: 0.5097 - val_acc: 0.8120\n",
      "Epoch 120/120\n",
      "38357/38357 [==============================] - 45s 1ms/step - loss: 0.5042 - acc: 0.8170 - val_loss: 0.4961 - val_acc: 0.8010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f337c177908>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt_dir = \"./models/seqLSTM/\"\n",
    "model_path = wt_dir+'biLSTM'+'{epoch:03d}'+'.h5'\n",
    "tensorboard = TensorBoard(log_dir='./Graph')\n",
    "model_checkpoint = ModelCheckpoint(model_path, save_best_only =False, period =2, save_weights_only = False)\n",
    "# model.fit([try_head, try_body], \n",
    "#           try_label, \n",
    "#           epochs=30, \n",
    "#           validation_data=([try_head, try_body], try_label), \n",
    "#           batch_size=BATCH_SIZE,\n",
    "#           shuffle=True,\n",
    "#           callbacks = [model_checkpoint, tensorboard])\n",
    "model.fit([train_head, train_body], \n",
    "          train_label, \n",
    "          epochs=2*EPOCHS, \n",
    "          validation_data=([val_head, val_body], val_label), \n",
    "          batch_size=BATCH_SIZE,\n",
    "          shuffle = True, \n",
    "          callbacks=[model_checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word2idx, open(\"word2idx_GRU.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
