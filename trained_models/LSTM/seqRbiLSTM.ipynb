{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seqRbiLSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cs2u2TxcPvtX",
        "colab_type": "code",
        "outputId": "247280c2-a36c-4b06-9a16-488e78b1c054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "from csv import DictReader\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import concatenate, Embedding, Dense, Lambda, Dropout, CuDNNLSTM, Activation, LSTM, Flatten, Input, RepeatVector, TimeDistributed, Bidirectional\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "import codecs\n",
        "import pickle\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import keras.backend as K"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DI2qoYfCQDqn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LEN_HEAD = 100\n",
        "MAX_LEN_BODY = 500\n",
        "VOCAB_SIZE = 15000\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4MCzFGNdQKkZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_vocab(lst, vocab_size):\n",
        "    \"\"\"\n",
        "    lst: list of sentences\n",
        "    \"\"\"\n",
        "    vocabcount = Counter(w for txt in lst for w in txt.lower().split())\n",
        "    vocabcount = vocabcount.most_common(vocab_size)\n",
        "    word2idx = {}\n",
        "    idx2word = {}\n",
        "    for i, word in enumerate(vocabcount):\n",
        "        word2idx[word[0]] = i\n",
        "        idx2word[i] = word[0]\n",
        "    return word2idx, idx2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Y9EUlw-jMsV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cov2idx_unk(lst, word2idx):\n",
        "    output = []\n",
        "    for sentence in lst:\n",
        "        temp = []\n",
        "        for word in sentence.split():\n",
        "            if word in word2idx:\n",
        "                temp.append(word2idx[word])\n",
        "            else:\n",
        "                temp.append(word2idx['<unk>'])\n",
        "        temp.append(word2idx['<unk>'])\n",
        "        output.append(temp)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2EemVI3QOk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_seq(cov_lst, max_len=MAX_LEN_BODY):\n",
        "    \"\"\"\n",
        "    list of list of index converted from words\n",
        "    \"\"\"\n",
        "    pad_lst = pad_sequences(cov_lst, maxlen = max_len, padding='post')\n",
        "    return pad_lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W0PjDC5VQQpJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_ref = {'agree': 0, 'disagree': 1, 'discuss': 2, 'unrelated': 3}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQHjXhsujSsE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_train_unk(file_instances, file_bodies):\n",
        "    \"\"\"\n",
        "    article: the name of the article file\n",
        "    \"\"\"\n",
        "    \n",
        "    instance_lst = []\n",
        "    # Process file\n",
        "    with open(file_instances, \"r\", encoding='utf-8') as table:\n",
        "        r = DictReader(table)\n",
        "        for line in r:\n",
        "            instance_lst.append(line)\n",
        "            \n",
        "    body_lst = []\n",
        "    # Process file\n",
        "    with open(file_bodies, \"r\", encoding='utf-8') as table:\n",
        "        r = DictReader(table)\n",
        "        for line in r:\n",
        "            body_lst.append(line)\n",
        "    \n",
        "    heads = {}\n",
        "    bodies = {}\n",
        "    \n",
        "    for instance in instance_lst:\n",
        "        if instance['Headline'] not in heads:\n",
        "            head_id = len(heads)\n",
        "            heads[instance['Headline']] = head_id\n",
        "        instance['Body ID'] = int(instance['Body ID'])\n",
        "    for body in body_lst:\n",
        "        bodies[int(body['Body ID'])] = body['articleBody']\n",
        "            \n",
        "    headData = []\n",
        "    bodyData = []\n",
        "    labelData = []\n",
        "    for instance in instance_lst:\n",
        "        headData.append(instance['Headline'])\n",
        "        bodyData.append(bodies[instance['Body ID']])\n",
        "        labelData.append(label_ref[instance['Stance']])\n",
        "            \n",
        "    \n",
        "    word2idx, idx2word = get_vocab(headData+bodyData, VOCAB_SIZE)\n",
        "    word2idx['<unk>'] = len(word2idx)\n",
        "    \n",
        "    cov_head = cov2idx_unk(headData, word2idx)\n",
        "    cov_body = cov2idx_unk(bodyData, word2idx)\n",
        "    remove_list = []\n",
        "    for i in range(len(cov_head)):\n",
        "        if len(cov_head[i])>MAX_LEN_HEAD or len(cov_body[i])>MAX_LEN_BODY:\n",
        "            remove_list.append(i)\n",
        "    for idx in sorted(remove_list, reverse = True):\n",
        "        cov_head.pop(idx)\n",
        "        cov_body.pop(idx)\n",
        "        labelData.pop(idx)\n",
        "    pad_head = pad_seq(cov_head, MAX_LEN_HEAD)\n",
        "    pad_body = pad_seq(cov_body, MAX_LEN_BODY)\n",
        "    return pad_head, pad_body, labelData, word2idx, idx2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6UYzleQDQVBw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pad_head, pad_body, labelData, word2idx, idx2word = load_train_unk(\"train_stances.csv\", \"train_bodies.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKYVXpTA5Q0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pickle.dump(word2idx, open(\"word2idx.pkl\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5WH1lFhQXl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for training\n",
        "train_head = pad_head[:-1000]\n",
        "train_body = pad_body[:-1000]\n",
        "train_label = labelData[:-1000]\n",
        "\n",
        "val_head = pad_head[-1000:]\n",
        "val_body = pad_body[-1000:]\n",
        "val_label = labelData[-1000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VTw9W_wDRdbQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "NUM_LAYERS = 0\n",
        "HIDDEN_DIM =  512\n",
        "EPOCHS = 60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YlMG8jV0Rgb0",
        "colab_type": "code",
        "outputId": "2d7da492-8885-4d44-9047-92ce2f1484dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "input_head = Input(shape=(MAX_LEN_HEAD,), dtype='int32', name='input_head')\n",
        "embed_head = Embedding(output_dim=EMBEDDING_DIM, input_dim=VOCAB_SIZE, input_length=MAX_LEN_HEAD)(input_head)\n",
        "embed_head = Lambda(lambda x: K.reverse(x, 1))(embed_head)\n",
        "# embed_head = Embedding(VOCAB_SIZE, EMBEDDING_DIM , input_length = MAX_LEN_HEAD, weights = [g_word_embedding_matrix], trainable=False)\n",
        "input_body = Input(shape=(MAX_LEN_BODY,), dtype='int32', name='input_body')\n",
        "embed_body = Embedding(output_dim=EMBEDDING_DIM, input_dim=VOCAB_SIZE, input_length=MAX_LEN_BODY)(input_body)\n",
        "# embed_body = Embedding(VOCAB_SIZE, EMBEDDING_DIM , input_length = MAX_LEN_BODY, weights = [g_word_embedding_matrix], trainable=False)\n",
        "\n",
        "concat = concatenate([embed_head, embed_body], axis = 1)\n",
        "\n",
        "x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(concat)\n",
        "x = Bidirectional(CuDNNLSTM(32))(x)\n",
        "\n",
        "# x = LSTM(32)(concat)\n",
        "# We stack a deep densely-connected network on top\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "\n",
        "# And finally we add the main logistic regression layer\n",
        "main_output = Dense(4, activation='softmax', name='main_output')(x)\n",
        "model = Model(inputs=[input_head, input_body], outputs=main_output)\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_head (InputLayer)         (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 100, 300)     4500000     input_head[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "input_body (InputLayer)         (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 100, 300)     0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 500, 300)     4500000     input_body[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 600, 300)     0           lambda_1[0][0]                   \n",
            "                                                                 embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 600, 64)      85504       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 64)           25088       bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64)           4160        bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64)           4160        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 64)           4160        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "main_output (Dense)             (None, 4)            260         dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 9,123,332\n",
            "Trainable params: 9,123,332\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fAuyaMCtRiqD",
        "colab_type": "code",
        "outputId": "8a9e45f2-e458-4145-8731-110ad0594dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4298
        }
      },
      "cell_type": "code",
      "source": [
        "wt_dir = \"./\"\n",
        "model_path = wt_dir+'seqRbiLSTM'+'{epoch:03d}'+'.h5'\n",
        "model_checkpoint = ModelCheckpoint(model_path, save_best_only =False, period =1, save_weights_only = False)\n",
        "model.fit([train_head, train_body], \n",
        "          train_label, \n",
        "          epochs=2*EPOCHS, \n",
        "          validation_data=([val_head, val_body], val_label), \n",
        "          batch_size=BATCH_SIZE,\n",
        "          shuffle = True, \n",
        "          callbacks=[model_checkpoint])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 38357 samples, validate on 1000 samples\n",
            "Epoch 1/120\n",
            "38357/38357 [==============================] - 95s 2ms/step - loss: 0.8040 - acc: 0.7380 - val_loss: 0.8785 - val_acc: 0.7400\n",
            "Epoch 2/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.7885 - acc: 0.7401 - val_loss: 0.7737 - val_acc: 0.7400\n",
            "Epoch 3/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.7876 - acc: 0.7392 - val_loss: 0.7727 - val_acc: 0.7400\n",
            "Epoch 4/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.7876 - acc: 0.7400 - val_loss: 0.7776 - val_acc: 0.7400\n",
            "Epoch 5/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.7859 - acc: 0.7398 - val_loss: 0.7672 - val_acc: 0.7400\n",
            "Epoch 6/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.7800 - acc: 0.7401 - val_loss: 0.7485 - val_acc: 0.7400\n",
            "Epoch 7/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.7671 - acc: 0.7460 - val_loss: 0.7624 - val_acc: 0.7440\n",
            "Epoch 8/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.7475 - acc: 0.7514 - val_loss: 0.7157 - val_acc: 0.7600\n",
            "Epoch 9/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.7146 - acc: 0.7708 - val_loss: 0.7218 - val_acc: 0.7700\n",
            "Epoch 10/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.7064 - acc: 0.7774 - val_loss: 0.7000 - val_acc: 0.7740\n",
            "Epoch 11/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.6957 - acc: 0.7843 - val_loss: 0.7013 - val_acc: 0.7720\n",
            "Epoch 12/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.6946 - acc: 0.7882 - val_loss: 0.7140 - val_acc: 0.7720\n",
            "Epoch 13/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.6798 - acc: 0.7827 - val_loss: 0.6737 - val_acc: 0.7760\n",
            "Epoch 14/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.6475 - acc: 0.7941 - val_loss: 0.6629 - val_acc: 0.7730\n",
            "Epoch 15/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.6353 - acc: 0.8017 - val_loss: 0.6536 - val_acc: 0.7830\n",
            "Epoch 16/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.6056 - acc: 0.8086 - val_loss: 0.6223 - val_acc: 0.7800\n",
            "Epoch 17/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.5752 - acc: 0.8137 - val_loss: 0.6125 - val_acc: 0.7970\n",
            "Epoch 18/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.5549 - acc: 0.8181 - val_loss: 0.5587 - val_acc: 0.7970\n",
            "Epoch 19/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.5362 - acc: 0.8224 - val_loss: 0.5424 - val_acc: 0.8080\n",
            "Epoch 20/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.5220 - acc: 0.8252 - val_loss: 0.5306 - val_acc: 0.8050\n",
            "Epoch 21/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.5133 - acc: 0.8274 - val_loss: 0.5312 - val_acc: 0.8100\n",
            "Epoch 22/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.5096 - acc: 0.8278 - val_loss: 0.5407 - val_acc: 0.8070\n",
            "Epoch 23/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.5011 - acc: 0.8294 - val_loss: 0.5447 - val_acc: 0.8040\n",
            "Epoch 24/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4983 - acc: 0.8299 - val_loss: 0.5171 - val_acc: 0.8180\n",
            "Epoch 25/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4950 - acc: 0.8314 - val_loss: 0.5098 - val_acc: 0.8120\n",
            "Epoch 26/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4880 - acc: 0.8328 - val_loss: 0.5350 - val_acc: 0.8090\n",
            "Epoch 27/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4799 - acc: 0.8331 - val_loss: 0.5054 - val_acc: 0.8090\n",
            "Epoch 28/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4712 - acc: 0.8352 - val_loss: 0.4906 - val_acc: 0.8120\n",
            "Epoch 29/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4609 - acc: 0.8366 - val_loss: 0.4855 - val_acc: 0.8280\n",
            "Epoch 30/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.4478 - acc: 0.8402 - val_loss: 0.4734 - val_acc: 0.8270\n",
            "Epoch 31/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4330 - acc: 0.8445 - val_loss: 0.4897 - val_acc: 0.8250\n",
            "Epoch 32/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4239 - acc: 0.8490 - val_loss: 0.4617 - val_acc: 0.8260\n",
            "Epoch 33/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.4126 - acc: 0.8518 - val_loss: 0.4607 - val_acc: 0.8370\n",
            "Epoch 34/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.4056 - acc: 0.8540 - val_loss: 0.4532 - val_acc: 0.8330\n",
            "Epoch 35/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.3967 - acc: 0.8557 - val_loss: 0.4802 - val_acc: 0.8190\n",
            "Epoch 36/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.3923 - acc: 0.8586 - val_loss: 0.4506 - val_acc: 0.8320\n",
            "Epoch 37/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.3842 - acc: 0.8599 - val_loss: 0.4279 - val_acc: 0.8420\n",
            "Epoch 38/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.3769 - acc: 0.8625 - val_loss: 0.4340 - val_acc: 0.8430\n",
            "Epoch 39/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.3714 - acc: 0.8643 - val_loss: 0.4208 - val_acc: 0.8430\n",
            "Epoch 40/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.3651 - acc: 0.8678 - val_loss: 0.4402 - val_acc: 0.8470\n",
            "Epoch 41/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.3578 - acc: 0.8694 - val_loss: 0.4087 - val_acc: 0.8560\n",
            "Epoch 42/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.3524 - acc: 0.8723 - val_loss: 0.4150 - val_acc: 0.8540\n",
            "Epoch 43/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.3457 - acc: 0.8738 - val_loss: 0.4088 - val_acc: 0.8540\n",
            "Epoch 44/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.3386 - acc: 0.8776 - val_loss: 0.4190 - val_acc: 0.8550\n",
            "Epoch 45/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.3300 - acc: 0.8797 - val_loss: 0.4031 - val_acc: 0.8530\n",
            "Epoch 46/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.3234 - acc: 0.8806 - val_loss: 0.4308 - val_acc: 0.8540\n",
            "Epoch 47/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.3197 - acc: 0.8852 - val_loss: 0.4041 - val_acc: 0.8570\n",
            "Epoch 48/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.3174 - acc: 0.8877 - val_loss: 0.4180 - val_acc: 0.8660\n",
            "Epoch 49/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.3138 - acc: 0.8865 - val_loss: 0.3952 - val_acc: 0.8700\n",
            "Epoch 50/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.3008 - acc: 0.8918 - val_loss: 0.3969 - val_acc: 0.8610\n",
            "Epoch 51/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.2977 - acc: 0.8938 - val_loss: 0.4415 - val_acc: 0.8550\n",
            "Epoch 52/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.2906 - acc: 0.8955 - val_loss: 0.3899 - val_acc: 0.8760\n",
            "Epoch 53/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.2841 - acc: 0.8971 - val_loss: 0.4634 - val_acc: 0.8620\n",
            "Epoch 54/120\n",
            "38357/38357 [==============================] - 95s 2ms/step - loss: 0.2832 - acc: 0.8994 - val_loss: 0.4003 - val_acc: 0.8630\n",
            "Epoch 55/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2776 - acc: 0.9016 - val_loss: 0.4087 - val_acc: 0.8680\n",
            "Epoch 56/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2749 - acc: 0.9025 - val_loss: 0.4221 - val_acc: 0.8620\n",
            "Epoch 57/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2704 - acc: 0.9034 - val_loss: 0.3901 - val_acc: 0.8740\n",
            "Epoch 58/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2690 - acc: 0.9041 - val_loss: 0.3953 - val_acc: 0.8710\n",
            "Epoch 59/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2610 - acc: 0.9069 - val_loss: 0.4204 - val_acc: 0.8620\n",
            "Epoch 60/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2615 - acc: 0.9077 - val_loss: 0.3881 - val_acc: 0.8690\n",
            "Epoch 61/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2558 - acc: 0.9096 - val_loss: 0.4051 - val_acc: 0.8610\n",
            "Epoch 62/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2530 - acc: 0.9106 - val_loss: 0.4089 - val_acc: 0.8600\n",
            "Epoch 63/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2505 - acc: 0.9105 - val_loss: 0.4149 - val_acc: 0.8700\n",
            "Epoch 64/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.2470 - acc: 0.9118 - val_loss: 0.4273 - val_acc: 0.8680\n",
            "Epoch 65/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2481 - acc: 0.9140 - val_loss: 0.4421 - val_acc: 0.8350\n",
            "Epoch 66/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2409 - acc: 0.9150 - val_loss: 0.4108 - val_acc: 0.8610\n",
            "Epoch 67/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2383 - acc: 0.9167 - val_loss: 0.4510 - val_acc: 0.8630\n",
            "Epoch 68/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2371 - acc: 0.9163 - val_loss: 0.3899 - val_acc: 0.8650\n",
            "Epoch 69/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2328 - acc: 0.9178 - val_loss: 0.4291 - val_acc: 0.8610\n",
            "Epoch 70/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2335 - acc: 0.9167 - val_loss: 0.4525 - val_acc: 0.8640\n",
            "Epoch 71/120\n",
            "38357/38357 [==============================] - 93s 2ms/step - loss: 0.2294 - acc: 0.9204 - val_loss: 0.4419 - val_acc: 0.8540\n",
            "Epoch 72/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2298 - acc: 0.9197 - val_loss: 0.3910 - val_acc: 0.8780\n",
            "Epoch 73/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2223 - acc: 0.9222 - val_loss: 0.4041 - val_acc: 0.8670\n",
            "Epoch 74/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2210 - acc: 0.9212 - val_loss: 0.4238 - val_acc: 0.8670\n",
            "Epoch 75/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2216 - acc: 0.9214 - val_loss: 0.3720 - val_acc: 0.8730\n",
            "Epoch 76/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.2180 - acc: 0.9231 - val_loss: 0.4160 - val_acc: 0.8770\n",
            "Epoch 77/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2154 - acc: 0.9244 - val_loss: 0.4433 - val_acc: 0.8620\n",
            "Epoch 78/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2116 - acc: 0.9240 - val_loss: 0.5192 - val_acc: 0.8500\n",
            "Epoch 79/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2111 - acc: 0.9261 - val_loss: 0.4215 - val_acc: 0.8680\n",
            "Epoch 80/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.2134 - acc: 0.9261 - val_loss: 0.4385 - val_acc: 0.8640\n",
            "Epoch 81/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.2100 - acc: 0.9265 - val_loss: 0.4300 - val_acc: 0.8670\n",
            "Epoch 82/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.2057 - acc: 0.9292 - val_loss: 0.4350 - val_acc: 0.8580\n",
            "Epoch 83/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.2030 - acc: 0.9286 - val_loss: 0.4463 - val_acc: 0.8660\n",
            "Epoch 84/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.1999 - acc: 0.9303 - val_loss: 0.5094 - val_acc: 0.8570\n",
            "Epoch 85/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.2034 - acc: 0.9305 - val_loss: 0.4814 - val_acc: 0.8630\n",
            "Epoch 86/120\n",
            "38357/38357 [==============================] - 90s 2ms/step - loss: 0.1969 - acc: 0.9322 - val_loss: 0.4702 - val_acc: 0.8620\n",
            "Epoch 87/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.1978 - acc: 0.9314 - val_loss: 0.5193 - val_acc: 0.8480\n",
            "Epoch 88/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.1963 - acc: 0.9311 - val_loss: 0.4356 - val_acc: 0.8720\n",
            "Epoch 89/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.1909 - acc: 0.9333 - val_loss: 0.4753 - val_acc: 0.8830\n",
            "Epoch 90/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.1924 - acc: 0.9332 - val_loss: 0.4393 - val_acc: 0.8650\n",
            "Epoch 91/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.1901 - acc: 0.9345 - val_loss: 0.4605 - val_acc: 0.8680\n",
            "Epoch 92/120\n",
            "38357/38357 [==============================] - 92s 2ms/step - loss: 0.1896 - acc: 0.9349 - val_loss: 0.5224 - val_acc: 0.8590\n",
            "Epoch 93/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.1862 - acc: 0.9358 - val_loss: 0.4889 - val_acc: 0.8710\n",
            "Epoch 94/120\n",
            "38357/38357 [==============================] - 91s 2ms/step - loss: 0.1867 - acc: 0.9349 - val_loss: 0.5995 - val_acc: 0.8450\n",
            "Epoch 95/120\n",
            "38357/38357 [==============================] - 89s 2ms/step - loss: 0.1835 - acc: 0.9365 - val_loss: 0.4867 - val_acc: 0.8580\n",
            "Epoch 96/120\n",
            "38357/38357 [==============================] - 87s 2ms/step - loss: 0.1857 - acc: 0.9363 - val_loss: 0.5191 - val_acc: 0.8580\n",
            "Epoch 97/120\n",
            "38357/38357 [==============================] - 87s 2ms/step - loss: 0.1809 - acc: 0.9376 - val_loss: 0.4627 - val_acc: 0.8700\n",
            "Epoch 98/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.1785 - acc: 0.9376 - val_loss: 0.5002 - val_acc: 0.8780\n",
            "Epoch 99/120\n",
            "38357/38357 [==============================] - 88s 2ms/step - loss: 0.1762 - acc: 0.9395 - val_loss: 0.4905 - val_acc: 0.8610\n",
            "Epoch 100/120\n",
            "38357/38357 [==============================] - 87s 2ms/step - loss: 0.1758 - acc: 0.9385 - val_loss: 0.4281 - val_acc: 0.8680\n",
            "Epoch 101/120\n",
            "38357/38357 [==============================] - 87s 2ms/step - loss: 0.1779 - acc: 0.9386 - val_loss: 0.4850 - val_acc: 0.8580\n",
            "Epoch 102/120\n",
            "31488/38357 [=======================>......] - ETA: 15s - loss: 0.1762 - acc: 0.9394"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-64417012f1a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m           callbacks=[model_checkpoint])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JL3ch73lRkEr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}